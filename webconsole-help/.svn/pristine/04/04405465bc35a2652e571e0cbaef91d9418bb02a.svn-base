<!DOCTYPE html>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd" lang="en-us" xml:lang="en-us" class="HelpTopic" data-mc-search-type="Stem" data-mc-help-system-file-name="Default.xml" data-mc-path-to-help-system="../../" data-mc-target-type="WebHelp2" data-mc-runtime-file-type="Topic" data-mc-preload-images="false" data-mc-in-preview-mode="false" data-mc-medium="non-print" data-mc-toc-path="Getting Data into the System">
    <!-- saved from url=(0016)http://localhost -->
    <head>
        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><title>Transport examples</title>
        <link href="../../Skins/Default/Stylesheets/Slideshow.css" rel="stylesheet" />
        <link href="../../Skins/Default/Stylesheets/TextEffects.css" rel="stylesheet" />
        <link href="../../Skins/Default/Stylesheets/Topic.css" rel="stylesheet" />
        <link href="../resources/stylesheets/leidos.css" rel="stylesheet" />
        <script src="../../Resources/Scripts/custom.modernizr.js">
        </script>
        <script src="../../Resources/Scripts/jquery.min.js">
        </script>
        <script src="../../Resources/Scripts/foundation.min.js">
        </script>
        <script src="../../Resources/Scripts/plugins.min.js">
        </script>
        <script src="../../Resources/Scripts/require.min.js">
        </script>
        <script src="../../Resources/Scripts/require.config.js">
        </script>
        <script src="../../Resources/Scripts/MadCapAll.js">
        </script>
    </head>
    <body>
        <h2 class="Heading2"><a name="kanchor36"></a><a name="kanchor37"></a>Transport examples</h2>
        <p>Amazon’s S3™ service (Simple Storage Service™) is a cost-effective way to store all types of data for applications which require a resilient, scalable, and accessible medium for data exchange. Many companies use S3 for a wide range of applications, such as e-commerce, scientific computing, video/audio processing, financial data, etc. </p>
        <p><span class="VariablesProductName">DigitalEdge</span> includes the S3 File Transport which processes data in user account S3™ buckets. The S3 transport service is a good choice when you have one or more static files, such as legacy data, to get into <span class="VariablesProductName">DigitalEdge</span>. The S3 transport can be configured two different ways:</p>
        <ul>
            <li value="1">To read the S3™ bucket once - As soon as the file appears in the bucket, <span class="VariablesProductName">DigitalEdge</span> pushes the file to the  JMS queue. </li>
            <li value="2">To poll the S3™ bucket periodically - The transport checks the bucket regularly at a set time interval. It may locate multiple files over time with the same name. This is a good option to use when you have an existing system that is generating large files and you are adding <span class="VariablesProductName">DigitalEdge</span> to the system as the big data processor and analyzer. You can configure the transport to check the bucket for new data every several minutes, hours, or once a day, depending on your time-to-availability requirements.</li>
        </ul>
        <p>Using S3 as a data sharing medium makes it easy for organizations to integrate <span class="VariablesProductName">DigitalEdge</span> into existing systems and to leverage <span class="VariablesProductName">DigitalEdge</span>'s processing capabilities. You do not have to use a proprietary API to create a transport, you can quickly get data into <span class="VariablesProductName">DigitalEdge</span>, and you do not have to move data to a new database or repository. <span class="VariablesProductName">DigitalEdge</span> can be integrated into an existing system with very little effort, offering new and innovative ways to use your data and to create innovative analytical solutions. </p>
        <h3 class="Heading3">Relational database export processing</h3>
        <p>An organization might use an S3™ transport to add additional capabilities to an existing stack without migrating or replacing existing solutions. If you have a way to produce a consumable data stream, you can simply route a portion of the feed to an S3™ bucket. For example, you could create a scheduled job to produce CSV exports from your application’s RDBMS and to feed the data into <span class="VariablesProductName">DigitalEdge</span>. The CSV exports could be copied to S3, using existing capabilities or Open Source transfer solutions. Use the S3FileTransportService to migrate the constant stream of data into S3™ buckets for periodic <span class="VariablesProductName">DigitalEdge</span> processing.</p>
        <p class="Indent">
            <img src="../resources/images/operations/transportexamples3csv.png" class="regular" />
        </p>
        <h3 class="Heading3">Non-CSV processing</h3>
        <p>Although traditionally, CSV exports are a common mechanism for data exchange, the <span class="VariablesProductName">DigitalEdge</span> S3 transport can be used to process other forms of data with little effort. You can use S3™ buckets to process:</p>
        <ul>
            <li value="1">traditional office files such as word processor documents, PDFs, spreadsheets, etc. </li>
            <li value="2">audio or video data streamed in from an existing enterprise technology such as share drives, CMS systems, email servers, etc.</li>
        </ul>
        <p>While it may require a modest investment of time and effort to hook into existing platforms and create a stream or copy of the data to S3™ buckets, once accomplished, <span class="VariablesProductName">DigitalEdge</span> processing will be transparent in its application against a very large data set. </p>
        <p>For example, you could leverage <span class="VariablesProductName">DigitalEdge</span> to perform copyright infringement analysis on a corpus of intellectual proprietary data compared to a set of data from the competition. The data may be stored on a CMS system such as Drupal®, with a stream or copy placed somewhere accessible by <span class="VariablesProductName">DigitalEdge</span>’s processing pipeline. Since exporting Word and PDF documents from Drupal is a trivial task, you would perform a one-time export to S3 where the <span class="VariablesProductName">DigitalEdge</span> S3FileTransportService would find and process the data. </p>
        <p class="Indent">
            <img src="../resources/images/operations/transportexampledrupal.png" class="regular" />
        </p>
        <h3 class="Heading3">For more detailed information</h3>
        <p><i>Configuration Guide</i> - data modeling and system building</p>
        <p><i>Operations Guide</i> - managing systems, starting up systems</p>
        <p class="Front">&#160;</p>
        <p class="Front">&#160;</p>
        <p class="FooterNonPrint">For  Technical Support, contact: <span class="VariablesEmail">DigitalEdgeSupport@Leidos.com</span></p>
        <p class="FooterNonPrint">To submit ideas or feedback, go to https://www9.v1ideas.com/digitaledge/welcome</p>
        <p class="FooterNonPrint">© <span class="VariablesCompanyName">Leidos</span>. All rights reserved</p>
    </body>
</html>